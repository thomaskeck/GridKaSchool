{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mid Level Tensorflow API\n",
    "\n",
    "This exercise introduces the mid-level concepts and API of tensorflow.\n",
    "You will learn the following concepts and packages\n",
    "  - collections\n",
    "  - tf.layers\n",
    "  - tf.summary\n",
    "  - Tensorboard\n",
    "  - tf.losses\n",
    "  - regularizers\n",
    "  - initializers\n",
    "  - image recognition with convolutional neural networks (on MNIST)\n",
    "  \n",
    "Unfortunately the tensorflow API is not really stable and (imho) quite confusing.\n",
    "There are always multiple ways on how to implement something. I will try to keep it as simple as possible, by introducing one component at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collections\n",
    "\n",
    "Before we dive into the mid-level API, we first have to introduce another concept, which will become important: **collections**. \n",
    "\n",
    "Tensorflow automatically keeps track of important tensors for you. They are stored in so-called **collections** (technically just a Python `list`). The names of default collections are defined in `tf.GraphKeys`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'MyWeightVector:0' shape=(3,) dtype=int32_ref>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.Variable([1,2,3], name=\"MyWeightVector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'MyWeightVector:0' shape=(3,) dtype=int32_ref>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many packages we encounter below will automatically append the tensors you create to certain collections.\n",
    "In this way we don't have to keep track of the tensors ourselves.\n",
    "Typical examples are:\n",
    "  - trainable variables, which will be adjusted by the optimizer in `tf.GraphKeys.TRAINABLE_VARIABLES`\n",
    "  - regularization losses, which are stored in `tf.GraphKeys.REGULARIZATION_LOSSES`\n",
    "  - summaries used to evaluate the training, which are stored in `tf.GraphKeys.SUMMARIES`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>Exercise</h3>\n",
    "Execute the two cells above multiple times. What behaviour do you observe? Why do you have to be careful if you are using an interactive environment like `Jupyter`?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST\n",
    "We use the same dataset as in the low-level exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `tf.layers`\n",
    "\n",
    "The `tf.layers` package contains pre-defined layers commonly used in neural networks:\n",
    "  - convolution layers: `tf.layers.conv2d`\n",
    "  - pooling layers: `tf.layers.max_pooling2d`\n",
    "  - dense (or fully-connected) layers: `tf.layers.dense`\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/layers\n",
    "\n",
    "Using those layers we can easily build a more complex neural network without having to define the weight variables ourselves (like we did in the low-level exercise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "session = tf.InteractiveSession()\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "conv = tf.layers.conv2d(image, filters=10, kernel_size=[5, 5], padding=\"same\",\n",
    "                         activation=tf.nn.relu)\n",
    "pool = tf.layers.max_pooling2d(conv, pool_size=[3, 3], strides=3)\n",
    "\n",
    "flat = tf.layers.flatten(pool)\n",
    "logits = tf.layers.dense(flat, units=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.placeholder(tf.int64, [None])\n",
    "misclassification_rate = 1.0 - tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits, 1), y), tf.float32))\n",
    "loss_function = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "minimize = tf.train.GradientDescentOptimizer(0.5).minimize(loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  0.07099998\n"
     ]
    }
   ],
   "source": [
    "tf.global_variables_initializer().run()\n",
    "for i in range(100):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    session.run(minimize, feed_dict={x: batch_xs, y: batch_ys})\n",
    "#print(\"Train\", session.run(misclassification_rate, feed_dict={x: mnist.train.images, y: mnist.train.labels}))\n",
    "print(\"Test \", session.run(misclassification_rate, feed_dict={x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>Exercise</h3>\n",
    "\n",
    "1. Change the number of filters and the kernel size of the convolution layers. <br>\n",
    "2. Add more convolution+pooling pairs before the dense layer.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tf.summary`\n",
    "\n",
    "During the training you can sample interesting quantities.\n",
    "Here we define two summary objects:\n",
    "  - a scalar summary (`tf.summary.scalar`), which records the misclassification rate during the training\n",
    "  - a histogram summary (`tf.summary.scalar`), which records the distribution of the maximum probability of each sample (a measure on \"how confident\" our network is in its decision)\n",
    "  \n",
    "Every summary you define is automatically added to the `tf.GraphKeys.SUMMARIES` collection of tensorflow.\n",
    "The convenience function `tf.summary.merge_all()` merges all summaries from the `tf.GraphKeys.SUMMARIES` collection into a single operations, which calculates them all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'summary/misclassification_rate:0' shape=() dtype=string>,\n",
       " <tf.Tensor 'summary/max_probability:0' shape=() dtype=string>]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.summary.scalar('summary/misclassification_rate', misclassification_rate)\n",
    "tf.summary.histogram('summary/max_probability', tf.reduce_max(tf.nn.softmax(logits), axis=1))\n",
    "tf.get_collection(tf.GraphKeys.SUMMARIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summaries should be written into files, here we use `tf.summary.FileWriter`. Tensorflow provides an external browser-based tool called `tensorboard`, which can read in those files and visualize your summaries. The tool can also be used to monitor a running training. Keep in mind that in real world examples people train for days and even weeks. Hence, simple print statements in a loop won't do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_writer = tf.summary.FileWriter('train_log')\n",
    "train_writer.add_graph(tf.get_default_graph())\n",
    "test_writer = tf.summary.FileWriter('test_log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run our training like before, but every 10 iterations we also run our summary operation on the entire training and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run()\n",
    "for i in range(100):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    session.run(minimize, feed_dict={x: batch_xs, y: batch_ys})\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        train_summary = session.run(summary, feed_dict={x: mnist.train.images[:1000], y: mnist.train.labels[:1000]})\n",
    "        train_writer.add_summary(train_summary, i)\n",
    "        test_summary = session.run(summary, feed_dict={x: mnist.test.images[:1000], y: mnist.test.labels[:1000]})\n",
    "        test_writer.add_summary(test_summary, i)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Tensorboard`\n",
    "\n",
    "It is important to be able to efficiently inspect and visualize the learning process of a neural network.\n",
    "Tensorflow includes a browser-based application called **tensorboard** which allows you to easily visualize the `tf.Summary` objects you sampled during the training.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>Exercise</h3>\n",
    "You can now start tensorboard and inspect your training by executing the following command in `bash` <br>\n",
    "<pre> tensorboard --logdir=\".\" </pre>\n",
    "\n",
    "Afterwards open your browser at:\n",
    "http://localhost:6006\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>Exercise</h3>\n",
    "In addition to the current summaries: visualize the loss function (scalar) and the distribution of the weights (histogram) in tensorboard.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `tf.losses`\n",
    "\n",
    "The `tf.losses` package contains pre-defined loss-functions commonly used in neural networks.\n",
    "In contrast to the low-level API, the loss is automatically averaged over the current batch.\n",
    "\n",
    "Every loss you define is automatically added to the `tf.GraphKeys.LOSSES` collection of tensorflow.\n",
    "Every regularization loss you define is automatically added to the `tf.GraphKeys.REGULARIZATION_LOSSES` collection of tensorflow.\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/losses\n",
    "\n",
    "In the code below I added some `tf.contrib.layers.l2_regularizer` terms. \n",
    "Those regularization terms will be automatically picked up by our loss-function below, because they are added to the correpsonding collection!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'conv2d/kernel/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
       " <tf.Tensor 'dense/kernel/Regularizer/l2_regularizer:0' shape=() dtype=float32>]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "session = tf.InteractiveSession()\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "conv = tf.layers.conv2d(image, filters=10, kernel_size=[5, 5], padding=\"same\",\n",
    "                         activation=tf.nn.relu,\n",
    "                         kernel_regularizer=tf.contrib.layers.l2_regularizer(0.1),\n",
    "                        )\n",
    "pool = tf.layers.max_pooling2d(conv, pool_size=[3, 3], strides=3)\n",
    "\n",
    "flat = tf.layers.flatten(pool)\n",
    "logits = tf.layers.dense(flat, units=10,\n",
    "                         kernel_regularizer=tf.contrib.layers.l2_regularizer(0.1),\n",
    "                         )\n",
    "tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the appropriate loss is now pretty easy. `tf.losses.sparse_softmax_cross_entropy` does exactly what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'sparse_softmax_cross_entropy_loss/value:0' shape=() dtype=float32>]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = tf.placeholder(tf.int64, [None])\n",
    "tf.losses.sparse_softmax_cross_entropy(labels=y, logits=logits)\n",
    "tf.get_collection(tf.GraphKeys.LOSSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convenience function `tf.losses.get_total_loss` returns the total loss in your graph, including potential regularization terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimize = tf.train.GradientDescentOptimizer(0.5).minimize(tf.losses.get_total_loss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.summary.scalar('summary/regularization_loss', tf.reduce_sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES), 0))\n",
    "tf.summary.scalar('summary/classifier_loss', tf.reduce_sum(tf.get_collection(tf.GraphKeys.LOSSES), 0))\n",
    "tf.summary.scalar('summary/total_loss', tf.losses.get_total_loss())\n",
    "summary = tf.summary.merge_all()\n",
    "\n",
    "train_writer = tf.summary.FileWriter('train_log')\n",
    "test_writer = tf.summary.FileWriter('test_log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run()\n",
    "for i in range(100):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    session.run(minimize, feed_dict={x: batch_xs, y: batch_ys})\n",
    "    if i % 10 == 0:\n",
    "        train_writer.add_summary(session.run(summary, feed_dict={x: mnist.train.images[:1000], y: mnist.train.labels[:1000]}), i)\n",
    "        test_writer.add_summary(session.run(summary, feed_dict={x: mnist.test.images[:1000], y: mnist.test.labels[:1000]}), i) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>Exercise</h3>\n",
    "Add an L1 regularization term to the biases using the `bias_regularizer` argument and the `tf.contrib.layers.l1_regularizer` class.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `tf.initializers`\n",
    "\n",
    "The initial values of the parameters of your model can be important for a successful training.\n",
    "A careful initalization ensures that the order of magnitude of: the activation during the forward pass and the gradients during the backward pass, does not change.\n",
    "\n",
    "Many different initalization schemes were proposed.\n",
    "They differ mostly in the **distribution** they use to sample the weights (usually either a normal distribution or a uniform distribution) and their **variance**.\n",
    "\n",
    "Two famous initalization schemes are:\n",
    "  - Glorot/Xavier initialization (default): $\\sigma =  \\sqrt{\\frac{2}{N_\\mathrm{in} + N_\\mathrm{out}}}$\n",
    "  - He initialization: $\\sigma =  \\sqrt{\\frac{2}{N_\\mathrm{in}}}$\n",
    "  \n",
    "We can easily change the used initialization, by passing an appropriate `Initializer` object as the `kernel_initializer` argument of a layer. In the next example we use the so-called **He Initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "session = tf.InteractiveSession()\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "conv = tf.layers.conv2d(image, filters=10, kernel_size=[5, 5], padding=\"same\",\n",
    "                         activation=tf.nn.relu,\n",
    "                         kernel_initializer=tf.initializers.variance_scaling(scale=2, mode='fan_in', distribution='normal'),\n",
    "                        )\n",
    "pool = tf.layers.max_pooling2d(conv, pool_size=[3, 3], strides=3)\n",
    "\n",
    "flat = tf.layers.flatten(pool)\n",
    "logits = tf.layers.dense(flat, units=10,\n",
    "                         kernel_initializer=tf.initializers.variance_scaling(scale=2, mode='fan_in', distribution='normal'),\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.placeholder(tf.int64, [None])\n",
    "tf.losses.sparse_softmax_cross_entropy(labels=y, logits=logits)\n",
    "minimize = tf.train.GradientDescentOptimizer(0.5).minimize(tf.losses.get_total_loss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.summary.scalar('summary/total_loss', tf.losses.get_total_loss())\n",
    "summary = tf.summary.merge_all()\n",
    "\n",
    "train_writer = tf.summary.FileWriter('train_log')\n",
    "test_writer = tf.summary.FileWriter('test_log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run()\n",
    "for i in range(100):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    session.run(minimize, feed_dict={x: batch_xs, y: batch_ys})\n",
    "    if i % 10 == 0:\n",
    "        train_writer.add_summary(session.run(summary, feed_dict={x: mnist.train.images[:1000], y: mnist.train.labels[:1000]}), i)\n",
    "        test_writer.add_summary(session.run(summary, feed_dict={x: mnist.test.images[:1000], y: mnist.test.labels[:1000]}), i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
